{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_A_DIR = './NumtaDB/training-a/'\n",
    "TRAINING_A_CSV = './NumtaDB/training-a.csv'\n",
    "\n",
    "TRAINING_B_DIR = './NumtaDB/training-b/'\n",
    "TRAINING_B_CSV = './NumtaDB/training-b.csv'\n",
    "\n",
    "TRAINING_C_DIR = './NumtaDB/training-c/'\n",
    "TRAINING_C_CSV = './NumtaDB/training-c.csv'\n",
    "\n",
    "TRAINING_D_DIR = './NumtaDB/training-d/'\n",
    "TRAINING_D_CSV = './NumtaDB/training-d.csv'\n",
    "\n",
    "TRAINING_E_DIR = './NumtaDB/training-e/'\n",
    "TRAINING_E_CSV = './NumtaDB/training-e.csv'\n",
    "\n",
    "DONT_INVERT_IMAGE_DIR = \"training-e\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INIT_FILE = 'model_desc.txt'\n",
    "IMAGE_DATASET_DIRS = [TRAINING_A_DIR, TRAINING_B_DIR, TRAINING_C_DIR]\n",
    "CSV_FILES = [TRAINING_A_CSV, TRAINING_B_CSV, TRAINING_C_CSV]\n",
    "MINI_BATCH_SIZE = 64\n",
    "IMAGE_DIM = 28 # Height and width of the image\n",
    "X_train = []\n",
    "y_train = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Resizing the image\n",
    "    img = cv2.resize(img,(IMAGE_DIM, IMAGE_DIM))\n",
    "    # Displaying the image\n",
    "    # plt.imshow(img, cmap='gray')\n",
    "    # plt.show()\n",
    "    # print(img.shape)\n",
    "    img = np.array(img)\n",
    "    img = img.astype('float32')\n",
    "    # Inverting the image\n",
    "    if DONT_INVERT_IMAGE_DIR not in path:\n",
    "        img = 255 - img\n",
    "    # dilation\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    img = cv2.dilate(img, kernel, iterations=1)\n",
    "    # plt.imshow(img, cmap='gray')\n",
    "    # plt.show()\n",
    "    # print(img.shape)\n",
    "    img /= 255\n",
    "    # reshaping the image\n",
    "    img = img.reshape(IMAGE_DIM, IMAGE_DIM, 1) # 1 for grayscale\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "class CustomImageDataset:\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, self.img_labels.columns.get_loc('filename')])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, self.img_labels.columns.get_loc('digit')] # 3 is the column index of the label\n",
    "        #one hot encoding\n",
    "        label = np.eye(10)[label]\n",
    "        \n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        # if self.target_transform:\n",
    "        #     label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader:\n",
    "    def __init__(self, dataset, batch_size=32, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.current_idx = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "        \n",
    "        # Get the next batch.\n",
    "        if self.current_idx + self.batch_size > len(self.dataset):\n",
    "            batch = [self.dataset[i] for i in range(self.current_idx, len(self.dataset))]\n",
    "            self.current_idx = len(self.dataset)\n",
    "        else:\n",
    "            batch = [self.dataset[i] for i in range(self.current_idx, self.current_idx + self.batch_size)]\n",
    "            self.current_idx += self.batch_size\n",
    "        \n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(batch)\n",
    "\n",
    "        images, labels = zip(*batch)\n",
    "        images = np.stack(images)\n",
    "        labels = np.stack(labels)\n",
    "\n",
    "        return images, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Images...\n",
      "Reading Images Completed\n",
      "(44359, 28, 28, 1) (44359, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading Images...\")\n",
    "for i, IMAGE_DATASET_DIR in enumerate(IMAGE_DATASET_DIRS):\n",
    "    CSV_FILE = CSV_FILES[i]\n",
    "    dataset = CustomImageDataset(annotations_file= CSV_FILE, img_dir=IMAGE_DATASET_DIR)\n",
    "    dataloader = CustomDataLoader(dataset, batch_size=MINI_BATCH_SIZE, shuffle=False)\n",
    "    for images, labels in dataloader:\n",
    "        # print(images.shape, labels.shape)\n",
    "        X_train.append(images)\n",
    "        y_train.append(labels)\n",
    "        # break\n",
    "    # break\n",
    "            \n",
    "print(\"Reading Images Completed\")\n",
    "\n",
    "X_train = np.concatenate(X_train)\n",
    "y_train = np.concatenate(y_train)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSES FOR CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SOFTMAX LAYER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done\n",
    "# https://stats.stackexchange.com/questions/304758/softmax-overflow?fbclid=IwAR0jL84MQqvY2Xk_CeBwaUM7kwtRBvV6O7yKtJhtcvGtawp0BwhKsZ4Buwk\n",
    "class Softmax_Layer:\n",
    "    def __init__(self):\n",
    "        self.layer_type = 'Softmax'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} Layer\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        x_max = np.max(X, axis=1)\n",
    "        x_max = x_max.reshape(x_max.shape[0], 1)\n",
    "        Z = np.exp(X-x_max)\n",
    "        # Z = np.exp(X)\n",
    "        sum = np.einsum('ij->i', Z)\n",
    "        sum = sum.reshape(sum.shape[0], 1)\n",
    "        return Z / sum\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        return np.copy(dZ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ReLU ACTIVATION </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done\n",
    "class ReLU_Activation:\n",
    "    def __init__(self):\n",
    "        self.layer_type = 'ReLU'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} Activation\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        Z = np.copy(X)\n",
    "        Z[Z < 0] = 0\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        dX = np.copy(self.X)\n",
    "        dX[dX < 0] = 0\n",
    "        dX[dX > 0] = 1\n",
    "        return dX * dZ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>FULLY CONNECTED LAYER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done\n",
    "class Fully_Connected_Layer:\n",
    "    def __init__(self, output_dim):\n",
    "        self.output_dim = output_dim\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Fully Connected Layer(output_dim={self.output_dim})\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "\n",
    "        if self.W is None:\n",
    "            self.W = np.random.randn(X.shape[1], self.output_dim) * math.sqrt(2 / X.shape[0])\n",
    "        \n",
    "        if self.b is None:\n",
    "            self.b = np.zeros((1, self.output_dim))\n",
    "\n",
    "        Z = np.einsum('ij,jk->ik', X, self.W) + self.b\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        dW = np.einsum('ij,ik->jk', self.X, dZ) / self.X.shape[0]\n",
    "        db = np.einsum('ij->j', dZ) / self.X.shape[0] \n",
    "        dX = np.einsum('ij,jk->ik', dZ, self.W.T)\n",
    "\n",
    "        self.W = self.W - learning_rate * dW\n",
    "        self.b = self.b - learning_rate * db\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>FLATENNING LAYER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatenning_Layer:\n",
    "    def __init__(self):\n",
    "        self.layer_type = 'Flatten'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} Layer\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input_shape = X.shape\n",
    "        # print(f\"input shape : {X.shape}\")\n",
    "        # print(f\"output shape : {X.reshape((X.shape[0], -1)).shape}\")\n",
    "        return X.reshape((X.shape[0], -1)) # check here\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        dX = np.copy(dZ)\n",
    "        return dX.reshape(self.input_shape) # check here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> WINDOWS: as_strided </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWindows(input, output_size, kernel_size, padding=0, stride=1, dilate=0):\n",
    "    working_input = input\n",
    "    working_pad = padding\n",
    "    \n",
    "    # dilate the input if necessary\n",
    "    if dilate != 0:\n",
    "        working_input = np.insert(working_input, range(1, input.shape[1]), 0, axis=1)\n",
    "        working_input = np.insert(working_input, range(1, input.shape[2]), 0, axis=2)\n",
    "\n",
    "    # pad the input if necessary\n",
    "    if working_pad != 0:\n",
    "        working_input = np.pad(working_input, pad_width=((0,), (working_pad,), (working_pad,), (0,)), mode='constant', constant_values=(0.,))\n",
    "\n",
    "    in_b, out_h, out_w, in_c = output_size\n",
    "    out_b, _, _, out_c = input.shape\n",
    "    batch_str, kern_h_str, kern_w_str, channel_str = working_input.strides\n",
    "\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        working_input,\n",
    "        (out_b, out_h, out_w, kernel_size, kernel_size, out_c),\n",
    "        (batch_str, stride * kern_h_str, stride * kern_w_str, kern_h_str, kern_w_str, channel_str)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MAX POOLING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class Max_Pooling:\n",
    "#     def __init__(self, filter_dim, stride):\n",
    "#         self.layer_type = 'Max Pooling'\n",
    "#         self.filter_dim = filter_dim\n",
    "#         self.stride = stride\n",
    "#         # self.X = None\n",
    "#         # self.Z_Max_idx = None\n",
    "    \n",
    "#     def __str__(self):\n",
    "#         return f\"{self.layer_type} (filter_dim={self.filter_dim}, stride={self.stride})\"\n",
    "    \n",
    "\n",
    "#     def forward(self, X):\n",
    "#         self.X_shape = X.shape\n",
    "#         n, h, w, c = X.shape\n",
    "#         new_h = (h - self.filter_dim) // self.stride + 1\n",
    "#         new_w = (w - self.filter_dim) // self.stride + 1\n",
    "        \n",
    "#         X_strided = getWindows(X, (n, new_h, new_w, c), self.filter_dim, stride=self.stride)\n",
    "        \n",
    "#         self.X_strided_shape = X_strided.shape\n",
    "\n",
    "#         Z = X_strided.max(axis=(3, 4))\n",
    "        \n",
    "#         self.Z_Max_idx = np.zeros(Z.shape, dtype=np.int32)\n",
    "\n",
    "#         for i in range(self.filter_dim):\n",
    "#             for j in range(self.filter_dim):\n",
    "#                 self.Z_Max_idx += (X_strided[:, :, :, i, j, :] == Z)\n",
    "\n",
    "#         return Z\n",
    "\n",
    "#     def backward(self, dZ, learning_rate=0.0001):\n",
    "#         # print(dZ.shape)\n",
    "#         n, h_new, w_new, c = dZ.shape\n",
    "#         dX = np.zeros(self.X_strided_shape)\n",
    "#         dZ_flat = dZ.ravel()\n",
    "\n",
    "#         for i in range(dZ_flat.shape[0]):\n",
    "#             max_idx = np.unravel_index(self.Z_Max_idx.flat[i], (n, h_new, w_new, self.filter_dim, self.filter_dim))\n",
    "#             # print(max_idx)\n",
    "#             dX[max_idx + (slice(None),)] = dZ_flat[i]\n",
    "#         # print(self.X_shape)\n",
    "#         # print(dX.shape)\n",
    "#         dX = dX.reshape(self.X_shape)\n",
    "#         return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new max pooling\n",
    "class Max_Pooling:\n",
    "    def __init__(self, filter_dim, stride):\n",
    "        self.layer_type = 'Max Pooling'\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        # self.X = None\n",
    "        # self.Z_Max_idx = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} (filter_dim={self.filter_dim}, stride={self.stride})\"\n",
    "    \n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X_shape = X.shape\n",
    "        n, h, w, c = X.shape\n",
    "        out_h = (h - self.filter_dim) // self.stride + 1\n",
    "        out_w = (w - self.filter_dim) // self.stride + 1\n",
    "        \n",
    "        X_strided = np.lib.stride_tricks.as_strided(\n",
    "            X,\n",
    "            shape=(n, out_h, out_w, self.filter_dim, self.filter_dim, c),\n",
    "            strides=(X.strides[0], self.stride * X.strides[1], self.stride * X.strides[2], X.strides[1], X.strides[2], X.strides[3]),\n",
    "            writeable=False\n",
    "        )\n",
    "        \n",
    "        out = np.max(X_strided, axis=(3, 4))\n",
    "\n",
    "        maxs = out.repeat(self.filter_dim, axis=1).repeat(self.filter_dim, axis=2)\n",
    "\n",
    "        x_window = X[:, :out_h * self.stride, :out_w * self.stride, :]\n",
    "\n",
    "        mask = np.equal(x_window, maxs).astype(int)\n",
    "\n",
    "        # self.X = X\n",
    "        self.mask = mask\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dA_prev, learning_rate=0.0001):\n",
    "        # print(dA_prev.shape)\n",
    "        mask = self.mask\n",
    "        dA = dA_prev.repeat(self.filter_dim, axis=1).repeat(self.filter_dim, axis=2)\n",
    "        dA = np.multiply(dA, mask)\n",
    "        pad = np.zeros(self.X_shape)\n",
    "        pad[:, :dA.shape[1], :dA.shape[2], :] = dA\n",
    "        \n",
    "        return pad  \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CONVOLUTION</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710\n",
    "# https://blog.ca.meron.dev/Vectorized-CNN/?fbclid=IwAR2GWeVd2AnOGLJrpDqNAFC6F2m5dhrNamB-km8y7D-0TKm4K7Uz1W-2L6Y\n",
    "class Convolution:\n",
    "    def __init__(self, num_output_channels, filter_dim, stride=1, padding=0):\n",
    "        self.layer_type = 'Convolution'\n",
    "        self.num_output_channels = num_output_channels\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} (num_output_channels={self.num_output_channels}, filter_dim={self.filter_dim}, stride={self.stride}, padding={self.padding})\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        n, h, w, c = X.shape\n",
    "        out_h = (h - self.filter_dim + 2*self.padding) // self.stride + 1\n",
    "        out_w = (w - self.filter_dim + 2*self.padding) // self.stride + 1\n",
    "\n",
    "        if self.W is None:\n",
    "            self.W = np.random.randn(self.num_output_channels, self.filter_dim, self.filter_dim, X.shape[3]) * math.sqrt(2 / X.shape[0])\n",
    "        if self.b is None:\n",
    "            self.b = np.zeros((self.num_output_channels))\n",
    "        \n",
    "        X_strided = getWindows(X, (n, out_h, out_w, c), self.filter_dim, self.padding, self.stride)\n",
    "        self.X_strided = X_strided\n",
    "\n",
    "        Z = np.einsum('ijklmn,olmn->ijko', X_strided, self.W) + self.b\n",
    "\n",
    "        return Z\n",
    "    \n",
    "        \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        \n",
    "        padding = self.filter_dim - 1 if self.padding == 0 else self.padding\n",
    "        dout_windows = getWindows(dZ, self.X.shape, self.filter_dim, padding=padding, stride=1, dilate=self.stride - 1)\n",
    "\n",
    "        rot_kern = np.rot90(self.W, 2, axes=(1, 2))\n",
    "\n",
    "        db = np.einsum('mijc->c', dZ)/dZ.shape[0]\n",
    "        dW = np.einsum('ijkmno,ijkl->lmno', self.X_strided, dZ)/dZ.shape[0]\n",
    "        dX = np.einsum('mhwijc,cijk->mhwk', dout_windows, rot_kern)\n",
    "    \n",
    "        self.W = self.W - learning_rate * dW\n",
    "        self.b = self.b - learning_rate * db\n",
    "        return dX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MODEL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, filePath):\n",
    "        self.layers = []\n",
    "        self.filePath = filePath\n",
    "        self.build_model()\n",
    "\n",
    "    def __str__(self):\n",
    "        string = 'MODEL DETAILS:\\n\\n'\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            string += f\"Layer {i+1}: {layer}\\n\"\n",
    "        return string\n",
    "    \n",
    "    def build_model(self):\n",
    "        #check if file exists\n",
    "        if not os.path.exists(self.filePath):\n",
    "            print('File does not exist')\n",
    "            return\n",
    "        with open(self.filePath, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "\n",
    "                line = line.strip()\n",
    "                \n",
    "                if line == '':\n",
    "                    continue\n",
    "\n",
    "                line_split = line.split(' ')\n",
    "                layer_name = str(line_split[0]).upper()\n",
    "                \n",
    "                if layer_name == 'FC':\n",
    "                    output_dim = int(line_split[1])\n",
    "                    self.layers.append(Fully_Connected_Layer(output_dim))\n",
    "\n",
    "                elif layer_name == 'CONV':\n",
    "                    num_output_channels = int(line_split[1])\n",
    "                    filter_dim = int(line_split[2])\n",
    "                    stride = int(line_split[3])\n",
    "                    padding = int(line_split[4])\n",
    "                    self.layers.append(Convolution(num_output_channels, filter_dim, stride, padding))\n",
    "\n",
    "                elif layer_name == 'MAXPOOL':\n",
    "                    filter_dim = int(line_split[1])\n",
    "                    stride = int(line_split[2])\n",
    "                    self.layers.append(Max_Pooling(filter_dim, stride))\n",
    "\n",
    "                elif layer_name == 'FLATTEN':\n",
    "                    self.layers.append(Flatenning_Layer())\n",
    "\n",
    "                elif layer_name == 'RELU':\n",
    "                    self.layers.append(ReLU_Activation())\n",
    "\n",
    "                elif layer_name == 'SOFTMAX':\n",
    "                    self.layers.append(Softmax_Layer())\n",
    "                \n",
    "                else:\n",
    "                    print('Invalid layer name')\n",
    "                    return\n",
    "        \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            # print(\"forward : \", layer)\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        for layer in reversed(self.layers):\n",
    "            # print(\"Backward : \",layer)\n",
    "            dZ = layer.backward(dZ, learning_rate)\n",
    "        return dZ\n",
    "    \n",
    "    def train(self, X, Y, X_val, y_val, learning_rate=0.0001, epochs=10, batch_size=64):\n",
    "        epochs = int(epochs)\n",
    "        results = []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                Y_batch = Y[i:i+batch_size]\n",
    "                Z = self.forward(X_batch)\n",
    "                dZ = Z - Y_batch\n",
    "                self.backward(dZ, learning_rate)\n",
    "                # print(f\"progress: {i+batch_size}/{X.shape[0]} completed. Loss: {self.loss(X_batch, Y_batch)}\")\n",
    "            train_loss = self.loss(X, Y)\n",
    "            train_acc, y_pred = self.evaluate(X, Y)\n",
    "            y_true = np.argmax(Y, axis=1)\n",
    "            F1_score = f1_score(y_true, y_pred, average='macro')\n",
    "            \n",
    "            val_loss = self.loss(X_val, y_val)\n",
    "            val_acc, _ = self.evaluate(X_val, y_val)\n",
    "            results.append([train_loss, train_acc, val_loss, val_acc, F1_score])\n",
    "            print(f\"Epoch {epoch+1} completed. Train Loss: {train_loss}, Train Accuracy: {train_acc}, f1 score: {F1_score},\\nValidation Loss: {val_loss} ,Validation Accuracy: {val_acc}\")\n",
    "            if val_acc>90:\n",
    "                print(\"\\n==================>>>Early Stopping. Validation Accuracy > 90% <<<=======================\\n\")\n",
    "                break\n",
    "        return results\n",
    "\n",
    "    def predict(self, X):\n",
    "        Z = self.forward(X)\n",
    "        return np.argmax(Z, axis=1)\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        Y_pred = self.predict(X)\n",
    "        Y_true = np.argmax(Y, axis=1)\n",
    "        return np.sum(Y_pred == Y_true) / len(Y_true) * 100, Y_pred\n",
    "    \n",
    "    def loss(self, X, Y):\n",
    "        Z = self.forward(X)\n",
    "        return -np.mean(Y * np.log(Z))\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39923, 28, 28, 1) (39923, 10) \n",
      " (4436, 28, 28, 1) (4436, 10)\n",
      "MODEL DETAILS:\n",
      "\n",
      "Layer 1: Convolution (num_output_channels=6, filter_dim=5, stride=1, padding=2)\n",
      "Layer 2: ReLU Activation\n",
      "Layer 3: Max Pooling (filter_dim=2, stride=2)\n",
      "Layer 4: Convolution (num_output_channels=16, filter_dim=5, stride=1, padding=0)\n",
      "Layer 5: ReLU Activation\n",
      "Layer 6: Max Pooling (filter_dim=2, stride=2)\n",
      "Layer 7: Flatten Layer\n",
      "Layer 8: Fully Connected Layer(output_dim=120)\n",
      "Layer 9: ReLU Activation\n",
      "Layer 10: Fully Connected Layer(output_dim=84)\n",
      "Layer 11: ReLU Activation\n",
      "Layer 12: Fully Connected Layer(output_dim=10)\n",
      "Layer 13: Softmax Layer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [04:23<2:07:12, 263.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Train Loss: 0.15283579246997867, Train Accuracy: 48.47832076747739, f1 score: 0.48428696365343515,\n",
      "Validation Loss: 0.14491133721314428 ,Validation Accuracy: 50.40577096483319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [08:58<2:06:04, 270.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Train Loss: 0.1856036929405518, Train Accuracy: 51.39393332164417, f1 score: 0.5216813215170091,\n",
      "Validation Loss: 0.09871485175994658 ,Validation Accuracy: 67.04238052299368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [13:59<2:08:02, 284.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Train Loss: 0.21619612122881054, Train Accuracy: 51.96753750970618, f1 score: 0.5355227314249411,\n",
      "Validation Loss: 0.08168359315708029 ,Validation Accuracy: 72.15960324616772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [18:27<2:00:26, 277.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed. Train Loss: 0.2250758301359734, Train Accuracy: 52.6738972522105, f1 score: 0.5471320834870695,\n",
      "Validation Loss: 0.0712390057542533 ,Validation Accuracy: 75.90171325518486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [22:42<1:52:23, 269.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed. Train Loss: 0.2348815277748981, Train Accuracy: 53.35270395511359, f1 score: 0.5556822450633601,\n",
      "Validation Loss: 0.06476473154538716 ,Validation Accuracy: 78.13345356176737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [26:59<1:46:08, 265.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 completed. Train Loss: 0.23277371022089177, Train Accuracy: 54.20183853918794, f1 score: 0.5661598507146289,\n",
      "Validation Loss: 0.05994803872360586 ,Validation Accuracy: 79.91433724075743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [31:05<1:39:19, 259.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 completed. Train Loss: 0.23477438051378297, Train Accuracy: 54.62014377677028, f1 score: 0.572199270332338,\n",
      "Validation Loss: 0.05641009425111715 ,Validation Accuracy: 81.2443642921551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [35:12<1:33:33, 255.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 completed. Train Loss: 0.23822232564942158, Train Accuracy: 54.42476767777973, f1 score: 0.5735045955255815,\n",
      "Validation Loss: 0.05361634342799119 ,Validation Accuracy: 82.05590622182146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [39:34<1:30:05, 257.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 completed. Train Loss: 0.23864298618090693, Train Accuracy: 55.01089597475139, f1 score: 0.5790807001454616,\n",
      "Validation Loss: 0.05178543082422439 ,Validation Accuracy: 82.70964833183048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [44:22<1:28:56, 266.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 completed. Train Loss: 0.2397126344544959, Train Accuracy: 55.216291360869675, f1 score: 0.5824976934031618,\n",
      "Validation Loss: 0.04996610276250967 ,Validation Accuracy: 83.273219116321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [48:35<1:23:09, 262.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 completed. Train Loss: 0.24583106753344694, Train Accuracy: 55.391628885604796, f1 score: 0.5851256139786059,\n",
      "Validation Loss: 0.048098317913823636 ,Validation Accuracy: 83.97204688908927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [52:46<1:17:41, 258.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 completed. Train Loss: 0.2416078023659926, Train Accuracy: 55.88257395486311, f1 score: 0.5904500969560258,\n",
      "Validation Loss: 0.04639819553590674 ,Validation Accuracy: 84.58070333633904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [56:52<1:12:17, 255.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 completed. Train Loss: 0.24165966100446054, Train Accuracy: 56.30087919244545, f1 score: 0.5952484080089078,\n",
      "Validation Loss: 0.044935511503950344 ,Validation Accuracy: 85.23444544634806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [1:01:02<1:07:36, 253.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 completed. Train Loss: 0.23971999460343582, Train Accuracy: 56.6690879943892, f1 score: 0.5995734799790279,\n",
      "Validation Loss: 0.04375775010651232 ,Validation Accuracy: 85.68530207394048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [1:05:14<1:03:14, 252.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 completed. Train Loss: 0.23945082646727484, Train Accuracy: 57.007238934949775, f1 score: 0.6041485457352664,\n",
      "Validation Loss: 0.04255577584482889 ,Validation Accuracy: 86.15870153291253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [1:09:23<58:43, 251.67s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 completed. Train Loss: 0.23447975751465616, Train Accuracy: 57.653482954687775, f1 score: 0.6105799517841254,\n",
      "Validation Loss: 0.041499536321348084 ,Validation Accuracy: 86.60955816050496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [1:13:40<54:55, 253.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 completed. Train Loss: 0.2314162286700598, Train Accuracy: 58.12438944969065, f1 score: 0.6151186160727262,\n",
      "Validation Loss: 0.040747881447950926 ,Validation Accuracy: 86.90261496844003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [1:18:31<52:56, 264.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 completed. Train Loss: 0.2322170606810165, Train Accuracy: 58.29972697442577, f1 score: 0.6178591371447493,\n",
      "Validation Loss: 0.039828210254542494 ,Validation Accuracy: 87.08295761947701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [1:22:44<47:52, 261.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 completed. Train Loss: 0.22965235116421928, Train Accuracy: 58.750594895173215, f1 score: 0.622239962329466,\n",
      "Validation Loss: 0.039377451843701075 ,Validation Accuracy: 87.35347159603246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [1:26:58<43:10, 259.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 completed. Train Loss: 0.22736596413356772, Train Accuracy: 59.226511033739946, f1 score: 0.6269200075062089,\n",
      "Validation Loss: 0.03879656702263325 ,Validation Accuracy: 87.64652840396754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [1:31:14<38:42, 258.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 completed. Train Loss: 0.2243364537166366, Train Accuracy: 59.75001878616336, f1 score: 0.631890710404021,\n",
      "Validation Loss: 0.03829780458125457 ,Validation Accuracy: 87.84941388638413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [1:35:22<34:01, 255.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 completed. Train Loss: 0.2193422442403261, Train Accuracy: 60.47641710292313, f1 score: 0.6385841338209799,\n",
      "Validation Loss: 0.0378795702209294 ,Validation Accuracy: 87.96212804328223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [1:39:28<29:27, 252.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 completed. Train Loss: 0.21586150297178816, Train Accuracy: 61.09009843949603, f1 score: 0.6440803832570242,\n",
      "Validation Loss: 0.03764988472008226 ,Validation Accuracy: 88.00721370604148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [1:43:33<25:00, 250.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 completed. Train Loss: 0.213251223571017, Train Accuracy: 61.53094707311575, f1 score: 0.6480794247037258,\n",
      "Validation Loss: 0.03732465645447753 ,Validation Accuracy: 88.00721370604148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [1:47:47<20:56, 251.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 completed. Train Loss: 0.21215557252891892, Train Accuracy: 61.86909801367633, f1 score: 0.6514500888048762,\n",
      "Validation Loss: 0.036901978950422014 ,Validation Accuracy: 87.93958521190261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [1:51:55<16:41, 250.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 completed. Train Loss: 0.2130484308829351, Train Accuracy: 62.16216216216216, f1 score: 0.6540244838084053,\n",
      "Validation Loss: 0.03666307213858073 ,Validation Accuracy: 88.16501352569884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [1:56:17<12:41, 253.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 completed. Train Loss: 0.20926956908095268, Train Accuracy: 62.84347368684718, f1 score: 0.6603776372631307,\n",
      "Validation Loss: 0.03626146430377382 ,Validation Accuracy: 88.27772768259693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [2:00:29<08:26, 253.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 completed. Train Loss: 0.20834202392891996, Train Accuracy: 63.26428374621146, f1 score: 0.6640757856158161,\n",
      "Validation Loss: 0.03608970397016151 ,Validation Accuracy: 88.32281334535618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [2:04:37<04:11, 251.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 completed. Train Loss: 0.20566468683245354, Train Accuracy: 63.70763720161311, f1 score: 0.6683348763747554,\n",
      "Validation Loss: 0.03587890350147402 ,Validation Accuracy: 88.48061316501352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [2:08:42<00:00, 257.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 completed. Train Loss: 0.20732508907029476, Train Accuracy: 63.76775292437943, f1 score: 0.6688988544701651,\n",
      "Validation Loss: 0.035938723964304864 ,Validation Accuracy: 88.48061316501352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, shuffle=False)\n",
    "print(X_train.shape, y_train.shape,\"\\n\", X_val.shape, y_val.shape)\n",
    "\n",
    "model = Model(MODEL_INIT_FILE)\n",
    "print(model)\n",
    "\n",
    "results = model.train(X_train, y_train, X_val, y_val, learning_rate=0.01, epochs=30, batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save output of model\n",
    "if len(results) != 0:\n",
    "    with open(\"1705027_output.txt\", \"w\") as file:\n",
    "        for result in results:\n",
    "            file.write(f\"{result[0]} {result[1]} {result[2]} {result[3]}\\n\")\n",
    "# to save model\n",
    "with open('1705027_model.pickle', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10908, 28, 28, 1) (10908, 10)\n",
      "Test Accuracy:  75.33920058672534\n"
     ]
    }
   ],
   "source": [
    "TEST_MODEL_PATH = './saved_model/model_cnn.pkl'\n",
    "TEST_DIR = TRAINING_D_DIR\n",
    "TEST_CSV = TRAINING_D_CSV\n",
    "TEST_DATA_PATH = TEST_DIR\n",
    "TEST_DATA_LABEL_PATH = TEST_CSV\n",
    "\n",
    "test_dataset = CustomImageDataset(annotations_file= TEST_DATA_LABEL_PATH, img_dir=TEST_DATA_PATH)\n",
    "test_dataloader = CustomDataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for images, labels in test_dataloader:\n",
    "    # print(images.shape, labels.shape)\n",
    "    X_test.append(images)\n",
    "    y_test.append(labels)\n",
    "\n",
    "X_test = np.concatenate(X_test)\n",
    "y_test = np.concatenate(y_test)\n",
    "print(X_test.shape, y_test.shape)\n",
    "# print(model.predict(images))\n",
    "acc, _ = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy: \", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfd499bf418f77ed98604f368b9dcc9d49d2a51ff3f93a138504985eb88a9fa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
