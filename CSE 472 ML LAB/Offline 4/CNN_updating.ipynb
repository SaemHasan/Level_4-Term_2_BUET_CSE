{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INIT_FILE = 'model_desc.txt'\n",
    "# IMAGE_DATASET_DIR = './TRAIN_IMAGES/'\n",
    "# CSV_FILE = 'training-a_revised.csv'\n",
    "IMAGE_DATASET_DIR = './NumtaDB/training-a/'\n",
    "CSV_FILE = './NumtaDB/training-a.csv'\n",
    "MINI_BATCH_SIZE = 64\n",
    "IMAGE_DIM = 28 # Height and width of the image\n",
    "\n",
    "# Dataset\n",
    "X_train = []\n",
    "y_train = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SOFTMAX LAYER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_Layer:\n",
    "    def __init__(self):\n",
    "        self.layer_type = 'Softmax'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} Layer\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        Z = np.exp(X)\n",
    "        return Z / np.einsum('ij->j', Z)\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        return np.copy(dZ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ReLU ACTIVATION </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU_Activation:\n",
    "    def __init__(self):\n",
    "        self.layer_type = 'ReLU'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} Activation\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "\n",
    "        Z = np.copy(X)\n",
    "        Z[Z < 0] = 0\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        dX = np.copy(self.X)\n",
    "\n",
    "        dX[dX < 0] = 0\n",
    "        dX[dX > 0] = 1\n",
    "        return dX * dZ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>FULLY CONNECTED LAYER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fully_Connected_Layer:\n",
    "    def __init__(self, output_dim):\n",
    "        self.layer_type = 'Fully Connected'\n",
    "        self.output_dim = output_dim\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Fully Connected Layer(output_dim={self.output_dim})\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "\n",
    "        if self.W is None:\n",
    "            self.W = np.random.randn(X.shape[1], self.output_dim) * math.sqrt(2 / X.shape[0])\n",
    "        \n",
    "        if self.b is None:\n",
    "            self.b = np.zeros((1, self.output_dim))\n",
    "\n",
    "        Z = np.einsum('ij,jk->ik', X, self.W) + self.b\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        dW = np.einsum('ij,ik->jk', self.X, dZ) / self.X.shape[0] # check here\n",
    "        db = np.einsum('ij->j', dZ) / self.X.shape[0] # check here\n",
    "        dX = np.einsum('ij,jk->ik', dZ, self.W.T)\n",
    "\n",
    "        self.W = self.W - learning_rate * dW\n",
    "        self.b = self.b - learning_rate * db\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>FLATENNING LAYER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatenning_Layer:\n",
    "    def __init__(self):\n",
    "        self.layer_type = 'Flatten'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} Layer\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input_shape = X.shape\n",
    "        return X.reshape((X.shape[0], -1)) # check here\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        dX = np.copy(dZ)\n",
    "        return dX.reshape(self.input_shape) # check here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MAX POOLING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Max_Pooling:\n",
    "    def __init__(self, filter_dim, stride):\n",
    "        self.layer_type = 'Max Pooling'\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        # self.X = None\n",
    "        # self.Z_Max_idx = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} (filter_dim={self.filter_dim}, stride={self.stride})\"\n",
    "    \n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X # delete this line\n",
    "        self.X_shape = X.shape\n",
    "        n, h, w, c = X.shape\n",
    "        new_h = (h - self.filter_dim) // self.stride + 1\n",
    "        new_w = (w - self.filter_dim) // self.stride + 1\n",
    "        \n",
    "        X_strided = np.lib.stride_tricks.as_strided(\n",
    "            X,\n",
    "            shape=(n, new_h, new_w, self.filter_dim, self.filter_dim, c),\n",
    "            strides=(X.strides[0], self.stride * X.strides[1], self.stride * X.strides[2], X.strides[1], X.strides[2], X.strides[3]),\n",
    "            writeable=False\n",
    "        )\n",
    "        \n",
    "        self.X_strided_shape = X_strided.shape\n",
    "\n",
    "        Z = X_strided.max(axis=(3, 4))\n",
    "        \n",
    "        self.Z_Max_idx = np.zeros(Z.shape, dtype=np.int32)\n",
    "\n",
    "        for i in range(self.filter_dim):\n",
    "            for j in range(self.filter_dim):\n",
    "                self.Z_Max_idx += (X_strided[:, :, :, i, j, :] == Z)\n",
    "\n",
    "        # self.Z_Max_idx = self.Z_Max_idx.repeat(self.filter_dim, axis=1).repeat(self.filter_dim, axis=2)\n",
    "\n",
    "        return Z\n",
    "    \n",
    "    # def forward_(self, X):\n",
    "    #     is_training = True\n",
    "    #     self.X_shape = X.shape\n",
    "    #     n, h, w, c = X.shape\n",
    "    #     new_h = (h - self.filter_dim) // self.stride + 1\n",
    "    #     new_w = (w - self.filter_dim) // self.stride + 1\n",
    "\n",
    "    #     windows = as_strided(X,\n",
    "    #                         shape=(n, new_h, new_w, self.filter_dim, self.filter_dim, c),\n",
    "    #                         strides=(X.strides[0], X.strides[1],\n",
    "    #                                 self.stride * X.strides[2],\n",
    "    #                                 self.stride * X.strides[3],\n",
    "    #                                 X.strides[2], X.strides[3])\n",
    "    #                         )\n",
    "\n",
    "    #     out = np.max(windows, axis=(3, 4))\n",
    "\n",
    "    #     maxs = out.repeat(self.stride, axis=1).repeat(self.stride, axis=2)\n",
    "    #     x_window = X[:, :new_h * self.stride, :new_w * self.stride, :]\n",
    "    #     mask = np.equal(x_window, maxs).astype(int)\n",
    "\n",
    "    #     if is_training:\n",
    "    #         self.X = X\n",
    "    #         self.Z_Max_idx = mask\n",
    "    #     return out\n",
    "\n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        \n",
    "        n, h_new, w_new, c = dZ.shape\n",
    "        dX = np.zeros(self.X_shape)\n",
    "        dZ_flat = dZ.ravel()\n",
    "        indices = np.indices((n, h_new, w_new, c))\n",
    "        indices = np.concatenate((indices, self.Z_Max_idx[..., None]), axis=0)\n",
    "        np.add.at(dX, tuple(indices), dZ_flat)\n",
    "\n",
    "        return dX\n",
    "    \n",
    "    def backward_stackoverflow(self, dZ, learning_rate=0.0001):\n",
    "        # mask = self.cache['mask']\n",
    "        print(f\"dZ shape = {dZ.shape}\")\n",
    "        dA = dZ.repeat(self.filter_dim, axis=1).repeat(self.filter_dim, axis=2)\n",
    "        print(f\"dA shape = {dA.shape}\")\n",
    "        print(f\"Z_Max_idx shape = {self.Z_Max_idx.shape}\")\n",
    "        dA = np.multiply(dA, self.Z_Max_idx)\n",
    "        pad = np.zeros(self.X_shape)\n",
    "        pad[:, :dA.shape[1], :dA.shape[2], :] = dA\n",
    "        return pad\n",
    "\n",
    "    def backward_(self, dZ, learning_rate=0.0001):\n",
    "        print(f\" dZ shape : {dZ.shape}\")\n",
    "        n, h_new, w_new, c = dZ.shape\n",
    "        dX = np.zeros(self.X_strided_shape)\n",
    "        dZ_flat = dZ.ravel()\n",
    "\n",
    "        for i in range(dZ_flat.shape[0]):\n",
    "            max_idx = np.unravel_index(self.Z_Max_idx.flat[i], (n, h_new, w_new, self.filter_dim, self.filter_dim))\n",
    "            # print(max_idx)\n",
    "            dX[max_idx + (slice(None),)] = dZ_flat[i]\n",
    "        # print(self.X_shape)\n",
    "        # print(dX.shape)\n",
    "        dX = dX.reshape(self.X_shape)\n",
    "        return dX\n",
    "\n",
    "    def backward1(self, del_v, lr):\n",
    "        del_u = np.zeros(self.X_shape)\n",
    "        \n",
    "        num_samples = del_v.shape[0]\n",
    "        input_dim = del_v.shape[1]\n",
    "        num_channels = del_v.shape[3]\n",
    "        \n",
    "        self.v_map = self.Z_Max_idx\n",
    "        self.kernel_size = self.filter_dim\n",
    "\n",
    "        for k in range(num_samples):\n",
    "            for l in range(num_channels):\n",
    "                for i in range(input_dim):\n",
    "                    for j in range(input_dim):\n",
    "                        position = tuple(sum(pos) for pos in zip((self.v_map[k, i, j, l] // self.kernel_size, self.v_map[k, i, j, l] % self.kernel_size), (i * self.stride, j * self.stride)))\n",
    "                        del_u[(k,) + position + (l,)] = del_u[(k,) + position + (l,)] + del_v[k, i, j, l]\n",
    "        \n",
    "        return del_u\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CONVOLUTION</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, num_output_channels, filter_dim, stride=1, padding=0):\n",
    "        self.layer_type = 'Convolution'\n",
    "        self.num_output_channels = num_output_channels\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} (num_output_channels={self.num_output_channels}, filter_dim={self.filter_dim}, stride={self.stride}, padding={self.padding})\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "\n",
    "        self.output_dim = (X.shape[1] - self.filter_dim + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        # padding\n",
    "        X = np.pad(X, ((0, 0), (self.padding, self.padding), (self.padding, self.padding), (0, 0)), 'constant')\n",
    "\n",
    "        Z = np.zeros((X.shape[0], self.output_dim, self.output_dim, self.num_output_channels))\n",
    "\n",
    "        if self.W is None:\n",
    "            self.W = np.random.randn(self.num_output_channels, self.filter_dim, self.filter_dim, X.shape[3]) * math.sqrt(2 / X.shape[0])\n",
    "        if self.b is None:\n",
    "            self.b = np.zeros((self.num_output_channels))\n",
    "        \n",
    "        # create strided view of the data\n",
    "        X_strided = np.lib.stride_tricks.as_strided(\n",
    "            X,\n",
    "            shape=(X.shape[0], self.output_dim, self.output_dim, self.filter_dim, self.filter_dim, X.shape[3]),\n",
    "            strides=(X.strides[0], self.stride * X.strides[1], self.stride * X.strides[2], X.strides[1], X.strides[2], X.strides[3])\n",
    "            )\n",
    "\n",
    "        # print(X_strided.shape)\n",
    "        # print(self.W.shape)\n",
    "        # sum over the filter dimensions\n",
    "        Z = np.einsum('ijklmn,olmn->ijko', X_strided, self.W) + self.b\n",
    "\n",
    "        return Z\n",
    "    \n",
    "    # def backward_(self, dZ, learning_rate=0.0001):\n",
    "    #     dW = np.zeros((self.num_output_channels, self.filter_dim, self.filter_dim, self.X.shape[3]))\n",
    "    #     db = np.zeros((self.num_output_channels))\n",
    "    #     dX = np.zeros(self.X.shape)\n",
    "\n",
    "    #     # shapes\n",
    "    #     print(f\"X: {self.X.shape}\")\n",
    "    #     print(f\"dZ: {dZ.shape}\")\n",
    "    #     print(f\"dW: {dW.shape}\")\n",
    "    #     print(f\"W: {self.W.shape}\")\n",
    "\n",
    "    #     for channel in range(self.num_output_channels):\n",
    "    #         for i in range(self.output_dim):\n",
    "    #             for j in range(self.output_dim):\n",
    "    #                 dW[channel, :, :, :] += np.sum(self.X[:, i*self.stride:i*self.stride+self.filter_dim, j*self.stride:j*self.stride+self.filter_dim, :] * dZ[:, i, j, channel].reshape((dZ.shape[0], 1, 1, 1)), axis=0)/dZ.shape[0]\n",
    "    #                 dX[:, i*self.stride:i*self.stride+self.filter_dim, j*self.stride:j*self.stride+self.filter_dim, :] += self.W[channel, :, :, :] * dZ[:, i, j, channel].reshape((dZ.shape[0], 1, 1, 1))/dZ.shape[0]\n",
    "    #                 db[channel] += np.sum(dZ[:, i, j, channel])\n",
    "        \n",
    "    #     self.W = self.W - learning_rate * dW\n",
    "    #     self.b = self.b - learning_rate * db\n",
    "\n",
    "    #     print(f\"dX: {dX.shape}\")\n",
    "    #     return dX\n",
    "        \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        # dW = np.zeros((self.num_output_channels, self.filter_dim, self.filter_dim, self.X.shape[3]))\n",
    "        # db = np.zeros((self.num_output_channels))\n",
    "        # dX = np.zeros(self.X.shape)\n",
    "    \n",
    "        X_pad = np.pad(self.X, ((0, 0), (self.padding, self.padding), (self.padding, self.padding), (0, 0)), 'constant')\n",
    "        X_strided = np.lib.stride_tricks.as_strided(X_pad, \n",
    "                    shape=(X_pad.shape[0], self.output_dim, self.output_dim, self.filter_dim, self.filter_dim, X_pad.shape[3]), \n",
    "                    strides=(X_pad.strides[0], self.stride * X_pad.strides[1], self.stride * X_pad.strides[2], X_pad.strides[1], X_pad.strides[2], X_pad.strides[3]))\n",
    "\n",
    "        # print shapes\n",
    "        # print(f\"X_strided: {X_strided.shape}\")\n",
    "        # print(f\"dZ: {dZ.shape}\")\n",
    "        # print(f\"W: {self.W.shape}\")\n",
    "\n",
    "        dW = np.einsum('ijkmno,ijkl->lmno', X_strided, dZ) / dZ.shape[0]\n",
    "        db = np.einsum('mijc->c', dZ) / dZ.shape[0]\n",
    "        dX_strided = np.einsum('oijl,mwho->mwhijl', self.W, dZ)\n",
    "    \n",
    "        self.W = self.W - learning_rate * dW\n",
    "        self.b = self.b - learning_rate * db\n",
    "\n",
    "\n",
    "        return dX_strided.sum(axis=(3, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MODEL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, filePath):\n",
    "        self.layers = []\n",
    "        self.filePath = filePath\n",
    "        self.build_model()\n",
    "\n",
    "    def __str__(self):\n",
    "        string = 'MODEL DETAILS:\\n\\n'\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            string += f\"Layer {i+1}: {layer}\\n\"\n",
    "        return string\n",
    "    \n",
    "    def build_model(self):\n",
    "        #check if file exists\n",
    "        if not os.path.exists(self.filePath):\n",
    "            print('File does not exist')\n",
    "            return\n",
    "        with open(self.filePath, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "\n",
    "                line = line.strip()\n",
    "                \n",
    "                if line == '':\n",
    "                    continue\n",
    "\n",
    "                line_split = line.split(' ')\n",
    "                layer_name = str(line_split[0]).upper()\n",
    "                \n",
    "                if layer_name == 'FC':\n",
    "                    output_dim = int(line_split[1])\n",
    "                    self.layers.append(Fully_Connected_Layer(output_dim))\n",
    "\n",
    "                elif layer_name == 'CONV':\n",
    "                    num_output_channels = int(line_split[1])\n",
    "                    filter_dim = int(line_split[2])\n",
    "                    stride = int(line_split[3])\n",
    "                    padding = int(line_split[4])\n",
    "                    self.layers.append(Convolution(num_output_channels, filter_dim, stride, padding))\n",
    "\n",
    "                elif layer_name == 'MAXPOOL':\n",
    "                    filter_dim = int(line_split[1])\n",
    "                    stride = int(line_split[2])\n",
    "                    self.layers.append(Max_Pooling(filter_dim, stride))\n",
    "\n",
    "                elif layer_name == 'FLATTEN':\n",
    "                    self.layers.append(Flatenning_Layer())\n",
    "\n",
    "                elif layer_name == 'RELU':\n",
    "                    self.layers.append(ReLU_Activation())\n",
    "\n",
    "                elif layer_name == 'SOFTMAX':\n",
    "                    self.layers.append(Softmax_Layer())\n",
    "                \n",
    "                else:\n",
    "                    print('Invalid layer name')\n",
    "                    return\n",
    "        \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            # print(\"forward : \", layer)\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        for layer in reversed(self.layers):\n",
    "            print(\"Backward : \",layer)\n",
    "            if isinstance(layer, Max_Pooling):\n",
    "                # calculate time taken for both methods\n",
    "                # start = time.time()\n",
    "                # dZ1 = layer.backward1(dZ, learning_rate)\n",
    "                # print(\"Time taken for backward method 1: \", time.time() - start)\n",
    "                # dZ = dZ1\n",
    "                start = time.time()\n",
    "                dZ2 = layer.backward_stackoverflow(dZ, learning_rate)\n",
    "                # dZ2 = layer.backward_(dZ, learning_rate)\n",
    "                # print(\"Time taken for backward method 2: \", time.time() - start)\n",
    "                # print(\"check \",np.allclose(dZ1, dZ2))\n",
    "                dZ = dZ2\n",
    "            else:\n",
    "                dZ = layer.backward(dZ, learning_rate)\n",
    "        return dZ\n",
    "    \n",
    "    def train(self, X, Y, learning_rate=0.0001, epochs=10, batch_size=64):\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                Y_batch = Y[i:i+batch_size]\n",
    "                Z = self.forward(X_batch)\n",
    "                dZ = Z - Y_batch\n",
    "                self.backward(dZ, learning_rate)\n",
    "            print(f\"Epoch {epoch+1} completed. Loss: {self.loss(X, Y)}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        Z = self.forward(X)\n",
    "        return np.argmax(Z, axis=1)\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        Y_pred = self.predict(X)\n",
    "        Y_true = np.argmax(Y, axis=1)\n",
    "        return np.sum(Y_pred == Y_true) / len(Y_true) * 100\n",
    "    \n",
    "    def loss(self, X, Y):\n",
    "        Z = self.forward(X)\n",
    "        return np.sum(-Y * np.log(Z))\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Resizing the image\n",
    "    img = cv2.resize(img,(IMAGE_DIM, IMAGE_DIM))\n",
    "    # reshaping the image\n",
    "    img = img.reshape(IMAGE_DIM, IMAGE_DIM, 1) # 1 for grayscale\n",
    "    # Displaying the image\n",
    "    # plt.imshow(img, cmap='gray')\n",
    "    # plt.show()\n",
    "    # print(img.shape)\n",
    "    img = np.array(img)\n",
    "    img = img.astype('float32')\n",
    "    # minus from 255\n",
    "    img = 255 - img\n",
    "    img /= 255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "class CustomImageDataset:\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 3] # 3 is the column index of the label\n",
    "        #one hot encoding\n",
    "        label = np.eye(10)[label]\n",
    "        \n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        # if self.target_transform:\n",
    "        #     label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader:\n",
    "    def __init__(self, dataset, batch_size=32, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.current_idx = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "        \n",
    "        # Get the next batch.\n",
    "        if self.current_idx + self.batch_size > len(self.dataset):\n",
    "            batch = [self.dataset[i] for i in range(self.current_idx, len(self.dataset))]\n",
    "            self.current_idx = len(self.dataset)\n",
    "        else:\n",
    "            batch = [self.dataset[i] for i in range(self.current_idx, self.current_idx + self.batch_size)]\n",
    "            self.current_idx += self.batch_size\n",
    "        \n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(batch)\n",
    "\n",
    "        images, labels = zip(*batch)\n",
    "        images = np.stack(images)\n",
    "        labels = np.stack(labels)\n",
    "\n",
    "        return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomImageDataset(annotations_file= CSV_FILE, img_dir=IMAGE_DATASET_DIR)\n",
    "dataloader = CustomDataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in dataloader:\n",
    "    # print(images.shape, labels.shape)\n",
    "    X_train.append(images)\n",
    "    y_train.append(labels)\n",
    "    break\n",
    "\n",
    "X_train = np.concatenate(X_train)\n",
    "y_train = np.concatenate(y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "model = Model(MODEL_INIT_FILE)\n",
    "print(model)\n",
    "\n",
    "model.train(X_train, y_train, learning_rate=0.001, epochs=2, batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODEL_PATH = './saved_model/model_cnn.pkl'\n",
    "TEST_DATA_PATH = './NumtaDB/training-b/'\n",
    "TEST_DATA_LABEL_PATH = './NumtaDB/training-b.csv'\n",
    "\n",
    "test_dataset = CustomImageDataset(annotations_file= TEST_DATA_LABEL_PATH, img_dir=TEST_DATA_PATH)\n",
    "test_dataloader = CustomDataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "for images, labels in test_dataloader:\n",
    "    # print(images.shape, labels.shape)\n",
    "    # print(model.predict(images))\n",
    "    print(model.evaluate(images, labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 | packaged by conda-forge | (main, Oct 25 2022, 06:21:25) [Clang 14.0.4 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfd499bf418f77ed98604f368b9dcc9d49d2a51ff3f93a138504985eb88a9fa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
