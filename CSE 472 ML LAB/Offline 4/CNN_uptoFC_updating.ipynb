{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SOFTMAX LAYER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_Layer:\n",
    "    def __init__(self):\n",
    "        self.layer_type = 'Softmax'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} Layer\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        Z = np.exp(X)\n",
    "        # print(f\"Z: {Z}\")\n",
    "        # print(f\"sum : {np.einsum('ij->i', Z)}\")\n",
    "        sum = np.einsum('ij->i', Z)\n",
    "        sum = sum.reshape(sum.shape[0], 1)\n",
    "        return Z / sum\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        return np.copy(dZ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ReLU ACTIVATION </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU_Activation:\n",
    "    def __init__(self):\n",
    "        self.layer_type = 'ReLU'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} Activation\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "\n",
    "        Z = np.copy(X)\n",
    "        Z[Z < 0] = 0\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        dX = np.copy(self.X)\n",
    "\n",
    "        dX[dX < 0] = 0\n",
    "        dX[dX > 0] = 1\n",
    "        return dX * dZ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>FULLY CONNECTED LAYER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Fully_Connected_Layer:\n",
    "    def __init__(self, output_dim):\n",
    "        self.output_dim = output_dim\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Fully Connected Layer(output_dim={self.output_dim})\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "\n",
    "        if self.W is None:\n",
    "            self.W = np.random.randn(X.shape[1], self.output_dim) * math.sqrt(2 / X.shape[0])\n",
    "        \n",
    "        if self.b is None:\n",
    "            self.b = np.zeros((1, self.output_dim))\n",
    "\n",
    "        Z = np.einsum('ij,jk->ik', X, self.W) + self.b\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        dW = np.einsum('ij,ik->jk', self.X, dZ) / self.X.shape[0] # check here\n",
    "        db = np.einsum('ij->j', dZ) / self.X.shape[0] # check here\n",
    "        dX = np.einsum('ij,jk->ik', dZ, self.W.T)\n",
    "\n",
    "        self.W = self.W - learning_rate * dW\n",
    "        self.b = self.b - learning_rate * db\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>FLATENNING LAYER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# build a NN model\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Model: {self.layers}\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        for layer in reversed(self.layers):\n",
    "            dZ = layer.backward(dZ, learning_rate)\n",
    "        return dZ\n",
    "    \n",
    "    def fit(self, X, Y, learning_rate=0.0001, epochs=100, batch_size=32, print_loss=False):\n",
    "        m = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, m, batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                Y_batch = Y[i:i+batch_size]\n",
    "                # print(X_batch.shape)\n",
    "                Z = self.forward(X_batch)\n",
    "                # print(Z.shape)\n",
    "                # print(Y_batch.shape)\n",
    "                dZ = Z - Y_batch\n",
    "                # print(\"dZ calc done in fit\")\n",
    "                self.backward(dZ, learning_rate)\n",
    "\n",
    "            if print_loss and epoch % 100 == 0:\n",
    "                loss = self.calculate_loss(X, Y)\n",
    "                print(f\"Loss after epoch {epoch}: {loss}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Z = self.forward(X)\n",
    "        # print(Z.shape)\n",
    "        # print(Z)\n",
    "        return np.argmax(Z, axis=1)+1\n",
    "    \n",
    "    def calculate_loss(self, X, Y):\n",
    "        m = X.shape[0]\n",
    "        # Y is one hot encoded\n",
    "        P = self.forward(X)\n",
    "        log_likelihood = -np.log(P[range(m), Y.argmax(axis=1)])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"./Toy Dataset/testNN.txt\")\n",
    "X = data[:, 0:4]\n",
    "\n",
    "# normalize the data\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "X= scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[:, 4]\n",
    "# print(Y.shape)\n",
    "# one hot encoding\n",
    "Y_one_hot = np.zeros((Y.shape[0], 4))\n",
    "for i in range(Y.shape[0]):\n",
    "    Y_one_hot[i, int(Y[i])-1] = 1\n",
    "Y_one_hot = Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic = Model()\n",
    "basic.add(Fully_Connected_Layer(6))\n",
    "basic.add(ReLU_Activation())\n",
    "basic.add(Fully_Connected_Layer(8))\n",
    "basic.add(ReLU_Activation())\n",
    "basic.add(Fully_Connected_Layer(5))\n",
    "basic.add(ReLU_Activation())\n",
    "basic.add(Fully_Connected_Layer(4))\n",
    "basic.add(Softmax_Layer())\n",
    "\n",
    "basic.fit(X, Y_one_hot, learning_rate=0.001, epochs=10001, batch_size=32, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.loadtxt(\"./Toy Dataset/testNN.txt\")\n",
    "test_X = test_data[:, 0:4]\n",
    "test_X = scaler.transform(test_X)\n",
    "test_Y = test_data[:, 4]\n",
    "test_Y = test_Y.astype(int)\n",
    "test_Y_one_hot = np.zeros((test_Y.shape[0], 4))\n",
    "for i in range(test_Y.shape[0]):\n",
    "    test_Y_one_hot[i, int(test_Y[i])-1] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y, Y_pred):\n",
    "    return np.sum(Y == Y_pred) / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using model\n",
    "predictions = basic.predict(test_X)\n",
    "# print(\"Y \\n\", test_Y)\n",
    "print(\"Predictions: \\n\", predictions)\n",
    "print(\"Accuracy: \", accuracy(test_Y, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfd499bf418f77ed98604f368b9dcc9d49d2a51ff3f93a138504985eb88a9fa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
