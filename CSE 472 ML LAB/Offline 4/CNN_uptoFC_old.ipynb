{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SOFTMAX LAYER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_Layer:\n",
    "    def __init__(self):\n",
    "        self.layer_type = 'Softmax'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} Layer\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        Z = np.exp(X)\n",
    "        return Z / np.einsum('ij->j', Z)\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        return np.copy(dZ)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ReLU ACTIVATION </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU_Activation:\n",
    "    def __init__(self):\n",
    "        self.layer_type = 'ReLU'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} Activation\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "\n",
    "        Z = np.copy(X)\n",
    "        Z[Z < 0] = 0\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        dX = np.copy(self.X)\n",
    "\n",
    "        dX[dX < 0] = 0\n",
    "        dX[dX > 0] = 1\n",
    "        return dX * dZ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>FULLY CONNECTED LAYER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Fully_Connected_Layer:\n",
    "    def __init__(self, output_dim):\n",
    "        self.output_dim = output_dim\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Fully_Connected_Layer(output_dim={self.output_dim})\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "\n",
    "        if self.W is None:\n",
    "            self.W = np.random.randn(X.shape[1], self.output_dim) * math.sqrt(2 / X.shape[0])\n",
    "        \n",
    "        if self.b is None:\n",
    "            self.b = np.zeros((1, self.output_dim))\n",
    "\n",
    "        Z = np.einsum('ij,jk->ik', X, self.W) + self.b\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        dW = np.einsum('ij,ik->jk', self.X, dZ) / self.X.shape[1] # check here\n",
    "        db = np.einsum('ij->j', dZ) / self.X.shape[0] # check here\n",
    "        dX = np.einsum('ij,jk->ik', dZ, self.W.T)\n",
    "\n",
    "        self.W = self.W - learning_rate * dW\n",
    "        self.b = self.b - learning_rate * db\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>FLATENNING LAYER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatenning_Layer:\n",
    "    def __init__(self):\n",
    "        self.layer_type = 'Flatten'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.layer_type} Layer\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input_shape = X.shape\n",
    "        return X.reshape((X.shape[0], -1)) # check here\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        dX = np.copy(dZ)\n",
    "        return dX.reshape(self.input_shape) # check here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# build a NN model\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Model: {self.layers}\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, dZ, learning_rate=0.0001):\n",
    "        for layer in reversed(self.layers):\n",
    "            dZ = layer.backward(dZ, learning_rate)\n",
    "        return dZ\n",
    "    \n",
    "    def fit(self, X, Y, learning_rate=0.0001, epochs=100, batch_size=32, print_loss=False):\n",
    "        m = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, m, batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                Y_batch = Y[i:i+batch_size]\n",
    "                # print(X_batch.shape)\n",
    "                Z = self.forward(X_batch)\n",
    "                # print(Z.shape)\n",
    "                # print(Y_batch.shape)\n",
    "                dZ = Z - Y_batch\n",
    "                # print(\"dZ calc done in fit\")\n",
    "                self.backward(dZ, learning_rate)\n",
    "\n",
    "            if print_loss and epoch % 100 == 0:\n",
    "                loss = self.calculate_loss(X, Y)\n",
    "                print(f\"Loss after epoch {epoch}: {loss}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Z = self.forward(X)\n",
    "        # print(Z.shape)\n",
    "        # print(Z)\n",
    "        return np.argmax(Z, axis=1)+1\n",
    "    \n",
    "    def calculate_loss(self, X, Y):\n",
    "        m = X.shape[0]\n",
    "        # Y is one hot encoded\n",
    "        P = self.forward(X)\n",
    "        log_likelihood = -np.log(P[range(m), Y.argmax(axis=1)])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.35320359,  1.36824164,  1.34494831,  1.36689489],\n",
       "       [ 0.5577194 ,  0.42359018,  0.45403648,  0.43159088],\n",
       "       [-0.39373132, -0.48317386, -0.46162906, -0.40522801],\n",
       "       ...,\n",
       "       [-0.39984943, -0.39227624, -0.44497152, -0.42093613],\n",
       "       [ 1.52610261,  1.35135152,  1.37790393,  1.35749308],\n",
       "       [-1.27088231, -1.35384488, -1.35058138, -1.32316619]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt(\"./Toy Dataset/testNN.txt\")\n",
    "X = data[:, 0:4]\n",
    "\n",
    "# normalize the data\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "X= scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[:, 4]\n",
    "# print(Y.shape)\n",
    "# one hot encoding\n",
    "Y_one_hot = np.zeros((Y.shape[0], 4))\n",
    "for i in range(Y.shape[0]):\n",
    "    Y_one_hot[i, int(Y[i])-1] = 1\n",
    "Y_one_hot = Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 6.202386346566593\n",
      "Loss after epoch 100: 6.198477630583524\n",
      "Loss after epoch 200: 6.194381263810961\n",
      "Loss after epoch 300: 6.190011376855234\n",
      "Loss after epoch 400: 6.184769501506302\n",
      "Loss after epoch 500: 6.178455115408696\n",
      "Loss after epoch 600: 6.171538668616236\n",
      "Loss after epoch 700: 6.164443286786684\n",
      "Loss after epoch 800: 6.156438712049125\n",
      "Loss after epoch 900: 6.147377475364741\n",
      "Loss after epoch 1000: 6.137167243545605\n"
     ]
    }
   ],
   "source": [
    "basic = Model()\n",
    "basic.add(Fully_Connected_Layer(6))\n",
    "basic.add(ReLU_Activation())\n",
    "basic.add(Fully_Connected_Layer(8))\n",
    "basic.add(ReLU_Activation())\n",
    "basic.add(Fully_Connected_Layer(5))\n",
    "basic.add(ReLU_Activation())\n",
    "basic.add(Fully_Connected_Layer(4))\n",
    "basic.add(Softmax_Layer())\n",
    "\n",
    "basic.fit(X, Y_one_hot, learning_rate=0.00001, epochs=1001, batch_size=32, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.loadtxt(\"./Toy Dataset/testNN.txt\")\n",
    "test_X = test_data[:, 0:4]\n",
    "test_X = scaler.transform(test_X)\n",
    "test_Y = test_data[:, 4]\n",
    "test_Y = test_Y.astype(int)\n",
    "test_Y_one_hot = np.zeros((test_Y.shape[0], 4))\n",
    "for i in range(test_Y.shape[0]):\n",
    "    test_Y_one_hot[i, int(test_Y[i])-1] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y, Y_pred):\n",
    "    return np.sum(Y == Y_pred) / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: \n",
      " [4 2 1 1 4 4 4 2 1 1 1 1 2 1 2 4 2 1 4 1 1 4 2 1 2 1 4 1 1 1 4 4 2 4 4 1 4\n",
      " 1 4 1 4 2 1 2 1 1 2 2 4 1 1 1 1 4 4 1 2 4 4 4 2 1 1 4 1 2 4 1 2 1 1 4 1 1\n",
      " 1 1 2 4 1 2 4 4 4 1 2 4 2 4 4 4 1 1 1 1 1 4 2 4 4 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 2 1 1 4 4 4 4 2 2 2 1 1 2 1 4 1 1 1 2 4 2 2 2 2 4 1 1 1 4 2 1 2 1 1 2 2 2\n",
      " 4 2 2 1 1 2 1 1 1 2 2 4 4 4 4 1 4 1 1 1 2 4 2 1 2 2 1 1 2 2 1 1 1 2 1 2 4\n",
      " 1 2 4 2 2 1 1 1 1 1 1 4 1 4 4 1 1 1 2 1 4 1 1 2 4 2 1 1 4 4 1 1 1 2 1 1 1\n",
      " 1 1 1 1 1 2 1 4 1 1 2 2 1 1 1 2 4 2 1 2 4 1 4 1 4 4 2 1 4 1 2 2 2 1 1 1 4\n",
      " 4 1 4 1 1 1 2 2 2 1 1 1 1 2 1 1 2 1 4 2 2 1 4 2 2 1 2 1 2 4 1 2 1 2 1 4 4\n",
      " 4 1 1 2 4 1 1 4 4 1 4 4 2 1 1 1 2 1 4 1 1 4 2 2 1 1 1 2 2 4 2 2 4 1 1 4 4\n",
      " 1 1 1 1 1 2 1 1 2 1 1 1 1 1 4 1 4 2 2 2 4 4 1 4 1 1 1 1 1 4 1 4 2 4 1 1 1\n",
      " 2 1 1 1 2 4 4 1 2 1 1 2 1 2 4 1 1 1 1 2 1 2 1 2 1 1 1 1 1 2 1 1 2 1 1 1 1\n",
      " 4 1 4 2 1 1 2 1 1 1 4 2 1 2 1 1 2 1 1 1 2 1 4 1 1 2 1 4 2 4 2 1 1 1 2 1 2\n",
      " 1 1 2 2 4 1 1 1 2 4 4 4 4 1 4 4 4 4 1 2 1 2 4 1 4 1 2 2 2 1 1 4 2 1 2 1 4\n",
      " 4 1 1 4 1 1 1 1 2 1 4 1 1 1 1 2 1 4 1]\n",
      "Accuracy:  0.49\n"
     ]
    }
   ],
   "source": [
    "# predict using model\n",
    "predictions = basic.predict(test_X)\n",
    "# print(\"Y \\n\", test_Y)\n",
    "print(\"Predictions: \\n\", predictions)\n",
    "print(\"Accuracy: \", accuracy(test_Y, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfd499bf418f77ed98604f368b9dcc9d49d2a51ff3f93a138504985eb88a9fa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
